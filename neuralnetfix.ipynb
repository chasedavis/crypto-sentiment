{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install tensorflow and protobuf -- run once!\n",
    "# anaconda install instructions: https://www.tensorflow.org/install/install_mac\n",
    "\n",
    "# # For chase's macbook, the following shell commands worked (take out ! if in terminal)\n",
    "# !conda create -n tensorflow python=2.7\n",
    "# !source activate tensorflow\n",
    "# !pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py2-none-any.whl\n",
    "\n",
    "# Remember to activate container when you want to use package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, csv, json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, json\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prev_price compound    neg    neu   pos next_price\n",
      "0     307.15   0.7424      0   0.69  0.31     307.26\n",
      "1        463  -0.7003  0.209  0.791     0     467.29\n",
      "2     465.01  -0.1759  0.253  0.747     0     461.57\n",
      "3     464.53        0      0      1     0     465.01\n",
      "4        462        0      0      1     0        465\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset Pickle\n",
    "import pickle\n",
    "import pandas as pd\n",
    "dfNN_raw = pickle.load(open('NN_pickles.p', 'rb'))\n",
    "print dfNN_raw.head(5)\n",
    "# pd.options.display.max_rows = len(dfNN_raw)\n",
    "# dfNN_raw.head(len(dfNN_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python\n",
    "# import tensorflow as tf\n",
    "# hello = tf.constant('Hello, TensorFlow!')\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prev_price   compound       neg       neu       pos next_price\n",
      "0  -0.205992   0.342107 -0.041533 -0.237517   0.30568   0.468625\n",
      "1   0.468625   -0.46241  0.214909  -0.11359 -0.110987   0.477325\n",
      "2   0.477325   -0.16998  0.268896 -0.167578 -0.110987   0.475247\n",
      "3   0.475247 -0.0718898 -0.041533  0.142851 -0.110987   0.464296\n",
      "4   0.464296 -0.0718898 -0.041533  0.142851 -0.110987   0.477325 <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "dfNN = (dfNN_raw - dfNN_raw.mean()) / (dfNN_raw.max() - dfNN_raw.min())\n",
    "dfNN.reset_index(inplace=True)\n",
    "del dfNN['index']\n",
    "dfNN['next_price'] = dfNN['prev_price'].shift(-1)\n",
    "dfNN.head(5)\n",
    "print dfNN.head(5), type(dfNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNN = pd.DataFrame(dfNN)\n",
    "dfNN = dfNN.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total series length is: 6426\n",
      "The current configuration gives us 2142 batches of 1 observations each one looking 3 steps in the past\n"
     ]
    }
   ],
   "source": [
    "# Taking straight from LSTM Stock Model\n",
    "num_epochs = 1000\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "total_series_length = len(dfNN.index)\n",
    "\n",
    "truncated_backprop_length = 3 #The size of the sequence\n",
    "\n",
    "state_size = 12 #The number of neurons\n",
    "\n",
    "num_features = 4\n",
    "num_classes = 1 #[1,0]\n",
    "\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "\n",
    "min_test_size = 100\n",
    "\n",
    "print('The total series length is: %d' %total_series_length)\n",
    "print('The current configuration gives us %d batches of %d observations each one looking %d steps in the past' \n",
    "      %(num_batches,batch_size,truncated_backprop_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     prev_price   compound        neg         neu          pos next_price\n",
      "0     -0.205992   0.342107  -0.041533   -0.237517      0.30568   0.468625\n",
      "1      0.468625   -0.46241   0.214909    -0.11359    -0.110987   0.477325\n",
      "2      0.477325   -0.16998   0.268896   -0.167578    -0.110987   0.475247\n",
      "3      0.475247 -0.0718898  -0.041533    0.142851    -0.110987   0.464296\n",
      "4      0.464296 -0.0718898  -0.041533    0.142851    -0.110987   0.477325\n",
      "5      0.477325 -0.0146195  -0.041533    -0.17494     0.237131   0.472953\n",
      "6      0.472953 -0.0146195   0.203866   -0.368805      0.18068   0.462435\n",
      "7      0.462435 -0.0718898  -0.041533    0.142851    -0.110987   0.472953\n",
      "8      0.472953 -0.0718898  -0.041533    0.142851    -0.110987   0.472953\n",
      "9      0.472953   0.212845  -0.041533   0.0115631    0.0328301   0.457197\n",
      "10     0.457197 -0.0718898  -0.041533    0.142851    -0.110987   0.457197\n",
      "11     0.457197 -0.0718898  -0.041533    0.142851    -0.110987   0.531216\n",
      "12     0.531216   0.152228  -0.041533    0.034876   0.00729245   0.462435\n",
      "13     0.462435 -0.0718898  -0.041533    0.142851    -0.110987   0.477325\n",
      "14     0.477325 -0.0718898  -0.041533    0.142851    -0.110987   0.455639\n",
      "15     0.455639 -0.0718898  -0.041533    0.142851    -0.110987   0.457197\n",
      "16     0.457197 -0.0718898  -0.041533    0.142851    -0.110987   0.457197\n",
      "17     0.457197   0.265988  0.0750314   -0.419112     0.376916   0.462132\n",
      "18     0.462132 -0.0718898  -0.041533    0.142851    -0.110987   0.475247\n",
      "19     0.475247 -0.0718898  -0.041533    0.142851    -0.110987   0.462435\n",
      "20     0.462435 -0.0718898  -0.041533    0.142851    -0.110987   0.457197\n",
      "21     0.457197 -0.0718898  -0.041533    0.142851    -0.110987   0.542125\n",
      "22     0.542125 -0.0718898  -0.041533    0.142851    -0.110987   0.464296\n",
      "23     0.464296 -0.0718898  -0.041533    0.142851    -0.110987   0.461872\n",
      "24     0.461872   0.312886  -0.041533   -0.252241     0.321809   0.457197\n",
      "25     0.457197 -0.0718898  -0.041533    0.142851    -0.110987   0.462132\n",
      "26     0.462132 -0.0718898  -0.041533    0.142851    -0.110987   0.455639\n",
      "27     0.455639 -0.0718898  -0.041533    0.142851    -0.110987   0.477282\n",
      "28     0.477282 -0.0718898  -0.041533    0.142851    -0.110987   0.462435\n",
      "29     0.462435 -0.0718898  -0.041533    0.142851    -0.110987   0.462132\n",
      "...         ...        ...        ...         ...          ...        ...\n",
      "6396  -0.208763 -0.0718898  -0.041533    0.142851    -0.110987  -0.202053\n",
      "6397  -0.202053  0.0952369  -0.041533   0.0673914   -0.0283258  -0.194262\n",
      "6398  -0.194262  -0.208569    0.11859  -0.0172712    -0.110987  -0.193656\n",
      "6399  -0.193656  0.0914728  -0.041533  0.00788215    0.0368623  -0.193656\n",
      "6400  -0.193656  -0.170231  0.0449701  0.00849565   -0.0585678  -0.193656\n",
      "6401  -0.193656 -0.0718898  -0.041533    0.142851    -0.110987  -0.194522\n",
      "6402  -0.194522  0.0658211  -0.041533   -0.129541       0.1874  -0.194522\n",
      "6403  -0.194522  0.0952369  -0.041533   0.0710723   -0.0323581  -0.194522\n",
      "6404  -0.194522  0.0835264  -0.041533   0.0796613   -0.0417667  -0.194522\n",
      "6405  -0.194522 -0.0503646  -0.041533   0.0937717   -0.0572237  -0.194522\n",
      "6406  -0.194522  0.0793161  -0.041533    0.042238 -0.000772061  -0.194522\n",
      "6407  -0.194522   0.161653   0.060921   -0.157149     0.106083  -0.194522\n",
      "6408  -0.194522   0.034565  -0.041533     0.10052   -0.0646161  -0.194522\n",
      "6409  -0.194522 -0.0718898  -0.041533    0.142851    -0.110987  -0.194522\n",
      "6410  -0.194522   0.195474  -0.041533  -0.0780074     0.130948  -0.194522\n",
      "6411  -0.194522  0.0463313  -0.041533   0.0207656    0.0227494  -0.194522\n",
      "6412  -0.194522   0.189089  -0.041533  -0.0473326    0.0973462  -0.208676\n",
      "6413  -0.208676  0.0401693  -0.041533   0.0121766     0.032158  -0.208676\n",
      "6414  -0.208676   -0.50323   0.404479   -0.340584   -0.0699925  -0.208676\n",
      "6415  -0.208676  -0.341847   0.285461   -0.184142    -0.110987   -0.20188\n",
      "6416   -0.20188  0.0401693  -0.041533   0.0121766     0.032158   -0.20188\n",
      "6417   -0.20188   0.210949  -0.041533   -0.146106     0.205545   -0.20188\n",
      "6418   -0.20188 -0.0718898  -0.041533    0.142851    -0.110987   -0.21742\n",
      "6419   -0.21742   0.127162  -0.041533  -0.0111363    0.0576957   -0.21742\n",
      "6420   -0.21742 -0.0718898  -0.041533    0.142851    -0.110987   -0.21742\n",
      "6421   -0.21742   -0.23871  0.0198167   0.0815018    -0.110987   -0.21742\n",
      "6422   -0.21742 -0.0338025   0.133927   -0.114817   -0.0209334   -0.21742\n",
      "6423   -0.21742  0.0401693  -0.041533  0.00113369    0.0442548   -0.21742\n",
      "6424   -0.21742  0.0401693  -0.041533   0.0121766     0.032158   -0.21742\n",
      "6425   -0.21742 -0.0718898  -0.041533    0.142851    -0.110987   -0.21742\n",
      "\n",
      "[6416 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split Training-Test\n",
    "dfTrain = dfNN[dfNN.index < num_batches*batch_size*truncated_backprop_length]\n",
    "print dfTrain\n",
    "\n",
    "for i in range(min_test_size,len(dfNN.index)):\n",
    "    \n",
    "    if(i % truncated_backprop_length*batch_size == 0):\n",
    "        test_first_idx = len(dfNN.index)-i\n",
    "        break\n",
    "\n",
    "dfTest =  dfNN[dfNN.index >= test_first_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]]\n",
      "[-0.21741998280605976 -0.21741998280605976 -0.21741998280605976 ...,\n",
      " 0.47524731872627507 0.47732506091309879 0.46862451550577483]\n"
     ]
    }
   ],
   "source": [
    "xTrain = dfTrain[['prev_price','neu','neg','pos']].as_matrix()[::-1]\n",
    "yTrain = dfTrain['next_price'].as_matrix()[::-1]\n",
    "\n",
    "print xTrain\n",
    "print yTrain\n",
    "# xTrain = dfTrain[['prev_price','neu','neg','pos']]\n",
    "# yTrain = dfTrain['next_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6416, 4)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTest = dfTest[['prev_price','neu','neg','pos']].as_matrix()\n",
    "yTest = dfTest['next_price'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTest.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAE/CAYAAAAHcrQrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMhEREZEkoKRMREREJAkoKRMRERFJAv8fdxuvPRbDMIwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2a2ac110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig,ax = plt.subplots(2,1, figsize = (10,5))\n",
    "ax[0].plot(xTrain[:,0])\n",
    "ax[0].set_title('Train (' +str(len(xTrain))+' data points)')\n",
    "ax[0].set_xlabel('Date')\n",
    "ax[0].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax[0].xaxis.set_major_formatter(mdates.DateFormatter('%m.%d.%y'))\n",
    "\n",
    "ax[1].plot(xTest[:,0])\n",
    "ax[1].set_title('Test (' +str(len(xTest))+' data points)')\n",
    "\n",
    "# plt.figure(figsize=(25,5))\n",
    "# plt.plot(xTrain[:,0])\n",
    "# plt.title('Train (' +str(len(xTrain))+' data points)')\n",
    "# plt.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "# plt.xaxis.set_major_formatter(mdates.DateFormatter('%m.%d.%y'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,3))\n",
    "# plt.plot(xTest[:,0])\n",
    "# plt.title('Test (' +str(len(xTest))+' data points)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SETUP NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchX_placeholder = tf.placeholder(dtype=tf.float32,shape=[None,truncated_backprop_length,num_features],name='data_ph')\n",
    "batchY_placeholder = tf.placeholder(dtype=tf.float32,shape=[None,truncated_backprop_length,num_classes],name='target_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# irrelevant if no more classes (random array unif distribution) \n",
    "W2 = tf.Variable(initial_value=np.random.rand(state_size,num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(initial_value=np.random.rand(1,num_classes),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unpack and create edge from input to hidden layer\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward pass\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=state_size)\n",
    "\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell=cell,inputs=batchX_placeholder,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# talk about permutations and how data goes through nodes\n",
    "states_series = tf.transpose(states_series,[1,0,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# backwards pass starting point\n",
    "last_state = tf.gather(params=states_series,indices=states_series.get_shape()[0]-1)\n",
    "last_label = tf.gather(params=labels_series,indices=len(labels_series)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bp weights and biases\n",
    "weight = tf.Variable(tf.truncated_normal([state_size,num_classes]))\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.matmul(last_state,weight) + bias\n",
    "loss = tf.reduce_mean(tf.squared_difference(last_label,prediction))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0 - Loss: 0.065730\n",
      "Step 50 - Loss: 0.000025\n",
      "Step 100 - Loss: 0.000057\n",
      "Step 150 - Loss: 0.000007\n",
      "Step 200 - Loss: 0.000002\n",
      "Step 250 - Loss: 0.000011\n",
      "Step 300 - Loss: 0.000033\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000018\n",
      "Step 450 - Loss: 0.000090\n",
      "Step 500 - Loss: 0.000041\n",
      "Step 550 - Loss: 0.000161\n",
      "Step 600 - Loss: 0.000005\n",
      "Step 650 - Loss: 0.000128\n",
      "Step 700 - Loss: 0.000052\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000008\n",
      "Step 850 - Loss: 0.000009\n",
      "Step 900 - Loss: 0.000016\n",
      "Step 950 - Loss: 0.000151\n",
      "Step 1000 - Loss: 0.000064\n",
      "Step 1050 - Loss: 0.000387\n",
      "Step 1100 - Loss: 0.000417\n",
      "Step 1150 - Loss: 0.000010\n",
      "Step 1200 - Loss: 0.000001\n",
      "Step 1250 - Loss: 0.000014\n",
      "Step 1300 - Loss: 0.000015\n",
      "Step 1350 - Loss: 0.000069\n",
      "Step 1400 - Loss: 0.000049\n",
      "Step 1450 - Loss: 0.000596\n",
      "Step 1500 - Loss: 0.000364\n",
      "Step 1550 - Loss: 0.001512\n",
      "Step 1600 - Loss: 0.000000\n",
      "Step 1650 - Loss: 0.000006\n",
      "Step 1700 - Loss: 0.000045\n",
      "Step 1750 - Loss: 0.000246\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000002\n",
      "Step 1900 - Loss: 0.000173\n",
      "Step 1950 - Loss: 0.000052\n",
      "Step 2000 - Loss: 0.000126\n",
      "Step 2050 - Loss: 0.000709\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 1\n",
      "Step 0 - Loss: 0.038942\n",
      "Step 50 - Loss: 0.000001\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000022\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000001\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000001\n",
      "Step 400 - Loss: 0.000100\n",
      "Step 450 - Loss: 0.000081\n",
      "Step 500 - Loss: 0.000008\n",
      "Step 550 - Loss: 0.000030\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000015\n",
      "Step 700 - Loss: 0.000002\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000005\n",
      "Step 950 - Loss: 0.000062\n",
      "Step 1000 - Loss: 0.000004\n",
      "Step 1050 - Loss: 0.000055\n",
      "Step 1100 - Loss: 0.000063\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000002\n",
      "Step 1300 - Loss: 0.000001\n",
      "Step 1350 - Loss: 0.000008\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000176\n",
      "Step 1500 - Loss: 0.000049\n",
      "Step 1550 - Loss: 0.000043\n",
      "Step 1600 - Loss: 0.000000\n",
      "Step 1650 - Loss: 0.000000\n",
      "Step 1700 - Loss: 0.000014\n",
      "Step 1750 - Loss: 0.000092\n",
      "Step 1800 - Loss: 0.000035\n",
      "Step 1850 - Loss: 0.000000\n",
      "Step 1900 - Loss: 0.000007\n",
      "Step 1950 - Loss: 0.000007\n",
      "Step 2000 - Loss: 0.000004\n",
      "Step 2050 - Loss: 0.001280\n",
      "Step 2100 - Loss: 0.000013\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 2\n",
      "Step 0 - Loss: 0.006854\n",
      "Step 50 - Loss: 0.000038\n",
      "Step 100 - Loss: 0.000004\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000007\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000028\n",
      "Step 450 - Loss: 0.000025\n",
      "Step 500 - Loss: 0.000003\n",
      "Step 550 - Loss: 0.000008\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000003\n",
      "Step 700 - Loss: 0.000002\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000003\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000016\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000006\n",
      "Step 1100 - Loss: 0.000013\n",
      "Step 1150 - Loss: 0.000004\n",
      "Step 1200 - Loss: 0.000009\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000006\n",
      "Step 1450 - Loss: 0.000072\n",
      "Step 1500 - Loss: 0.000001\n",
      "Step 1550 - Loss: 0.000000\n",
      "Step 1600 - Loss: 0.000000\n",
      "Step 1650 - Loss: 0.000001\n",
      "Step 1700 - Loss: 0.000016\n",
      "Step 1750 - Loss: 0.000115\n",
      "Step 1800 - Loss: 0.000020\n",
      "Step 1850 - Loss: 0.000000\n",
      "Step 1900 - Loss: 0.000001\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000087\n",
      "Step 2050 - Loss: 0.001484\n",
      "Step 2100 - Loss: 0.000021\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 3\n",
      "Step 0 - Loss: 0.001079\n",
      "Step 50 - Loss: 0.000007\n",
      "Step 100 - Loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000011\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000000\n",
      "Step 450 - Loss: 0.000008\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000006\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000005\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000004\n",
      "Step 1000 - Loss: 0.000009\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000001\n",
      "Step 1150 - Loss: 0.000007\n",
      "Step 1200 - Loss: 0.000018\n",
      "Step 1250 - Loss: 0.000001\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000002\n",
      "Step 1400 - Loss: 0.000014\n",
      "Step 1450 - Loss: 0.000035\n",
      "Step 1500 - Loss: 0.000003\n",
      "Step 1550 - Loss: 0.000012\n",
      "Step 1600 - Loss: 0.000000\n",
      "Step 1650 - Loss: 0.000002\n",
      "Step 1700 - Loss: 0.000011\n",
      "Step 1750 - Loss: 0.000123\n",
      "Step 1800 - Loss: 0.000011\n",
      "Step 1850 - Loss: 0.000000\n",
      "Step 1900 - Loss: 0.000002\n",
      "Step 1950 - Loss: 0.000002\n",
      "Step 2000 - Loss: 0.000095\n",
      "Step 2050 - Loss: 0.001538\n",
      "Step 2100 - Loss: 0.000024\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 4\n",
      "Step 0 - Loss: 0.000113\n",
      "Step 50 - Loss: 0.000000\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000013\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000005\n",
      "Step 450 - Loss: 0.000003\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000000\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000009\n",
      "Step 750 - Loss: 0.000002\n",
      "Step 800 - Loss: 0.000005\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000001\n",
      "Step 1000 - Loss: 0.000016\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000008\n",
      "Step 1200 - Loss: 0.000023\n",
      "Step 1250 - Loss: 0.000003\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000002\n",
      "Step 1400 - Loss: 0.000019\n",
      "Step 1450 - Loss: 0.000024\n",
      "Step 1500 - Loss: 0.000007\n",
      "Step 1550 - Loss: 0.000017\n",
      "Step 1600 - Loss: 0.000000\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000006\n",
      "Step 1750 - Loss: 0.000125\n",
      "Step 1800 - Loss: 0.000008\n",
      "Step 1850 - Loss: 0.000000\n",
      "Step 1900 - Loss: 0.000004\n",
      "Step 1950 - Loss: 0.000005\n",
      "Step 2000 - Loss: 0.000067\n",
      "Step 2050 - Loss: 0.001543\n",
      "Step 2100 - Loss: 0.000026\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 5\n",
      "Step 0 - Loss: 0.000005\n",
      "Step 50 - Loss: 0.000001\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000014\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000016\n",
      "Step 450 - Loss: 0.000001\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000000\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000010\n",
      "Step 750 - Loss: 0.000002\n",
      "Step 800 - Loss: 0.000005\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000000\n",
      "Step 1000 - Loss: 0.000020\n",
      "Step 1050 - Loss: 0.000001\n",
      "Step 1100 - Loss: 0.000001\n",
      "Step 1150 - Loss: 0.000009\n",
      "Step 1200 - Loss: 0.000027\n",
      "Step 1250 - Loss: 0.000006\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000002\n",
      "Step 1400 - Loss: 0.000023\n",
      "Step 1450 - Loss: 0.000020\n",
      "Step 1500 - Loss: 0.000010\n",
      "Step 1550 - Loss: 0.000017\n",
      "Step 1600 - Loss: 0.000001\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000003\n",
      "Step 1750 - Loss: 0.000124\n",
      "Step 1800 - Loss: 0.000006\n",
      "Step 1850 - Loss: 0.000001\n",
      "Step 1900 - Loss: 0.000005\n",
      "Step 1950 - Loss: 0.000008\n",
      "Step 2000 - Loss: 0.000043\n",
      "Step 2050 - Loss: 0.001501\n",
      "Step 2100 - Loss: 0.000029\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 6\n",
      "Step 0 - Loss: 0.000185\n",
      "Step 50 - Loss: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000014\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000024\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000000\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000010\n",
      "Step 750 - Loss: 0.000002\n",
      "Step 800 - Loss: 0.000005\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000000\n",
      "Step 1000 - Loss: 0.000023\n",
      "Step 1050 - Loss: 0.000001\n",
      "Step 1100 - Loss: 0.000002\n",
      "Step 1150 - Loss: 0.000008\n",
      "Step 1200 - Loss: 0.000029\n",
      "Step 1250 - Loss: 0.000009\n",
      "Step 1300 - Loss: 0.000001\n",
      "Step 1350 - Loss: 0.000002\n",
      "Step 1400 - Loss: 0.000024\n",
      "Step 1450 - Loss: 0.000018\n",
      "Step 1500 - Loss: 0.000012\n",
      "Step 1550 - Loss: 0.000016\n",
      "Step 1600 - Loss: 0.000001\n",
      "Step 1650 - Loss: 0.000004\n",
      "Step 1700 - Loss: 0.000002\n",
      "Step 1750 - Loss: 0.000114\n",
      "Step 1800 - Loss: 0.000004\n",
      "Step 1850 - Loss: 0.000003\n",
      "Step 1900 - Loss: 0.000006\n",
      "Step 1950 - Loss: 0.000007\n",
      "Step 2000 - Loss: 0.000027\n",
      "Step 2050 - Loss: 0.001314\n",
      "Step 2100 - Loss: 0.000027\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 7\n",
      "Step 0 - Loss: 0.000744\n",
      "Step 50 - Loss: 0.000028\n",
      "Step 100 - Loss: 0.000003\n",
      "Step 150 - Loss: 0.000001\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000010\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000023\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000009\n",
      "Step 750 - Loss: 0.000002\n",
      "Step 800 - Loss: 0.000004\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000001\n",
      "Step 1000 - Loss: 0.000024\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000003\n",
      "Step 1150 - Loss: 0.000006\n",
      "Step 1200 - Loss: 0.000027\n",
      "Step 1250 - Loss: 0.000013\n",
      "Step 1300 - Loss: 0.000001\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000020\n",
      "Step 1450 - Loss: 0.000018\n",
      "Step 1500 - Loss: 0.000013\n",
      "Step 1550 - Loss: 0.000011\n",
      "Step 1600 - Loss: 0.000001\n",
      "Step 1650 - Loss: 0.000007\n",
      "Step 1700 - Loss: 0.000004\n",
      "Step 1750 - Loss: 0.000086\n",
      "Step 1800 - Loss: 0.000002\n",
      "Step 1850 - Loss: 0.000008\n",
      "Step 1900 - Loss: 0.000002\n",
      "Step 1950 - Loss: 0.000003\n",
      "Step 2000 - Loss: 0.000015\n",
      "Step 2050 - Loss: 0.000823\n",
      "Step 2100 - Loss: 0.000008\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 8\n",
      "Step 0 - Loss: 0.001873\n",
      "Step 50 - Loss: 0.000033\n",
      "Step 100 - Loss: 0.000004\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000004\n",
      "Step 350 - Loss: 0.000001\n",
      "Step 400 - Loss: 0.000015\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000004\n",
      "Step 600 - Loss: 0.000004\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000010\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000002\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000004\n",
      "Step 1000 - Loss: 0.000022\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000002\n",
      "Step 1150 - Loss: 0.000004\n",
      "Step 1200 - Loss: 0.000017\n",
      "Step 1250 - Loss: 0.000025\n",
      "Step 1300 - Loss: 0.000004\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000010\n",
      "Step 1450 - Loss: 0.000019\n",
      "Step 1500 - Loss: 0.000005\n",
      "Step 1550 - Loss: 0.000003\n",
      "Step 1600 - Loss: 0.000010\n",
      "Step 1650 - Loss: 0.000036\n",
      "Step 1700 - Loss: 0.000012\n",
      "Step 1750 - Loss: 0.000030\n",
      "Step 1800 - Loss: 0.000002\n",
      "Step 1850 - Loss: 0.000029\n",
      "Step 1900 - Loss: 0.000004\n",
      "Step 1950 - Loss: 0.000004\n",
      "Step 2000 - Loss: 0.000038\n",
      "Step 2050 - Loss: 0.000155\n",
      "Step 2100 - Loss: 0.000118\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 9\n",
      "Step 0 - Loss: 0.002315\n",
      "Step 50 - Loss: 0.000054\n",
      "Step 100 - Loss: 0.000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000001\n",
      "Step 400 - Loss: 0.000006\n",
      "Step 450 - Loss: 0.000002\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000003\n",
      "Step 600 - Loss: 0.000007\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000009\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000007\n",
      "Step 1000 - Loss: 0.000006\n",
      "Step 1050 - Loss: 0.000001\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000000\n",
      "Step 1250 - Loss: 0.000015\n",
      "Step 1300 - Loss: 0.000006\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000003\n",
      "Step 1450 - Loss: 0.000023\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000085\n",
      "Step 1600 - Loss: 0.000046\n",
      "Step 1650 - Loss: 0.000008\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000019\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000056\n",
      "Step 1900 - Loss: 0.000022\n",
      "Step 1950 - Loss: 0.000009\n",
      "Step 2000 - Loss: 0.000008\n",
      "Step 2050 - Loss: 0.000002\n",
      "Step 2100 - Loss: 0.000023\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 10\n",
      "Step 0 - Loss: 0.000617\n",
      "Step 50 - Loss: 0.000045\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000003\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000002\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000006\n",
      "Step 450 - Loss: 0.000017\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000005\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000002\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000002\n",
      "Step 850 - Loss: 0.000004\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000009\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000001\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000001\n",
      "Step 1200 - Loss: 0.000005\n",
      "Step 1250 - Loss: 0.000005\n",
      "Step 1300 - Loss: 0.000009\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000004\n",
      "Step 1450 - Loss: 0.000023\n",
      "Step 1500 - Loss: 0.000011\n",
      "Step 1550 - Loss: 0.000072\n",
      "Step 1600 - Loss: 0.000026\n",
      "Step 1650 - Loss: 0.000002\n",
      "Step 1700 - Loss: 0.000014\n",
      "Step 1750 - Loss: 0.000123\n",
      "Step 1800 - Loss: 0.000002\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000046\n",
      "Step 1950 - Loss: 0.000009\n",
      "Step 2000 - Loss: 0.000000\n",
      "Step 2050 - Loss: 0.000005\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 11\n",
      "Step 0 - Loss: 0.000326\n",
      "Step 50 - Loss: 0.000011\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000004\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000004\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000012\n",
      "Step 450 - Loss: 0.000032\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000004\n",
      "Step 650 - Loss: 0.000001\n",
      "Step 700 - Loss: 0.000002\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000002\n",
      "Step 850 - Loss: 0.000002\n",
      "Step 900 - Loss: 0.000002\n",
      "Step 950 - Loss: 0.000009\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000001\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000003\n",
      "Step 1200 - Loss: 0.000005\n",
      "Step 1250 - Loss: 0.000003\n",
      "Step 1300 - Loss: 0.000009\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000004\n",
      "Step 1450 - Loss: 0.000025\n",
      "Step 1500 - Loss: 0.000016\n",
      "Step 1550 - Loss: 0.000060\n",
      "Step 1600 - Loss: 0.000015\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000017\n",
      "Step 1750 - Loss: 0.000148\n",
      "Step 1800 - Loss: 0.000002\n",
      "Step 1850 - Loss: 0.000056\n",
      "Step 1900 - Loss: 0.000050\n",
      "Step 1950 - Loss: 0.000005\n",
      "Step 2000 - Loss: 0.000005\n",
      "Step 2050 - Loss: 0.000016\n",
      "Step 2100 - Loss: 0.000004\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 12\n",
      "Step 0 - Loss: 0.000455\n",
      "Step 50 - Loss: 0.000001\n",
      "Step 100 - Loss: 0.000003\n",
      "Step 150 - Loss: 0.000003\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000004\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000018\n",
      "Step 450 - Loss: 0.000037\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000003\n",
      "Step 600 - Loss: 0.000003\n",
      "Step 650 - Loss: 0.000002\n",
      "Step 700 - Loss: 0.000003\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000002\n",
      "Step 900 - Loss: 0.000002\n",
      "Step 950 - Loss: 0.000007\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000001\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000005\n",
      "Step 1200 - Loss: 0.000005\n",
      "Step 1250 - Loss: 0.000003\n",
      "Step 1300 - Loss: 0.000009\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000004\n",
      "Step 1450 - Loss: 0.000024\n",
      "Step 1500 - Loss: 0.000015\n",
      "Step 1550 - Loss: 0.000059\n",
      "Step 1600 - Loss: 0.000009\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000016\n",
      "Step 1750 - Loss: 0.000138\n",
      "Step 1800 - Loss: 0.000002\n",
      "Step 1850 - Loss: 0.000055\n",
      "Step 1900 - Loss: 0.000043\n",
      "Step 1950 - Loss: 0.000003\n",
      "Step 2000 - Loss: 0.000009\n",
      "Step 2050 - Loss: 0.000030\n",
      "Step 2100 - Loss: 0.000010\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 13\n",
      "Step 0 - Loss: 0.000596\n",
      "Step 50 - Loss: 0.000000\n",
      "Step 100 - Loss: 0.000003\n",
      "Step 150 - Loss: 0.000003\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000004\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000023\n",
      "Step 450 - Loss: 0.000035\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000004\n",
      "Step 600 - Loss: 0.000003\n",
      "Step 650 - Loss: 0.000002\n",
      "Step 700 - Loss: 0.000003\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000002\n",
      "Step 950 - Loss: 0.000005\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000001\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000005\n",
      "Step 1200 - Loss: 0.000004\n",
      "Step 1250 - Loss: 0.000002\n",
      "Step 1300 - Loss: 0.000009\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000004\n",
      "Step 1450 - Loss: 0.000022\n",
      "Step 1500 - Loss: 0.000012\n",
      "Step 1550 - Loss: 0.000062\n",
      "Step 1600 - Loss: 0.000007\n",
      "Step 1650 - Loss: 0.000004\n",
      "Step 1700 - Loss: 0.000014\n",
      "Step 1750 - Loss: 0.000123\n",
      "Step 1800 - Loss: 0.000001\n",
      "Step 1850 - Loss: 0.000055\n",
      "Step 1900 - Loss: 0.000038\n",
      "Step 1950 - Loss: 0.000002\n",
      "Step 2000 - Loss: 0.000008\n",
      "Step 2050 - Loss: 0.000039\n",
      "Step 2100 - Loss: 0.000014\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 14\n",
      "Step 0 - Loss: 0.000643\n",
      "Step 50 - Loss: 0.000001\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000003\n",
      "Step 300 - Loss: 0.000001\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000025\n",
      "Step 450 - Loss: 0.000033\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000004\n",
      "Step 600 - Loss: 0.000003\n",
      "Step 650 - Loss: 0.000002\n",
      "Step 700 - Loss: 0.000003\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000004\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000004\n",
      "Step 1200 - Loss: 0.000004\n",
      "Step 1250 - Loss: 0.000002\n",
      "Step 1300 - Loss: 0.000008\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000004\n",
      "Step 1450 - Loss: 0.000020\n",
      "Step 1500 - Loss: 0.000010\n",
      "Step 1550 - Loss: 0.000066\n",
      "Step 1600 - Loss: 0.000006\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000012\n",
      "Step 1750 - Loss: 0.000113\n",
      "Step 1800 - Loss: 0.000001\n",
      "Step 1850 - Loss: 0.000056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1900 - Loss: 0.000034\n",
      "Step 1950 - Loss: 0.000002\n",
      "Step 2000 - Loss: 0.000006\n",
      "Step 2050 - Loss: 0.000042\n",
      "Step 2100 - Loss: 0.000016\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 15\n",
      "Step 0 - Loss: 0.000635\n",
      "Step 50 - Loss: 0.000004\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000003\n",
      "Step 300 - Loss: 0.000001\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000025\n",
      "Step 450 - Loss: 0.000032\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000004\n",
      "Step 600 - Loss: 0.000003\n",
      "Step 650 - Loss: 0.000002\n",
      "Step 700 - Loss: 0.000003\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000004\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000004\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000002\n",
      "Step 1300 - Loss: 0.000007\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000003\n",
      "Step 1450 - Loss: 0.000018\n",
      "Step 1500 - Loss: 0.000009\n",
      "Step 1550 - Loss: 0.000067\n",
      "Step 1600 - Loss: 0.000006\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000010\n",
      "Step 1750 - Loss: 0.000106\n",
      "Step 1800 - Loss: 0.000001\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000032\n",
      "Step 1950 - Loss: 0.000001\n",
      "Step 2000 - Loss: 0.000005\n",
      "Step 2050 - Loss: 0.000043\n",
      "Step 2100 - Loss: 0.000016\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 16\n",
      "Step 0 - Loss: 0.000613\n",
      "Step 50 - Loss: 0.000007\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000003\n",
      "Step 300 - Loss: 0.000002\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000025\n",
      "Step 450 - Loss: 0.000031\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000004\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000002\n",
      "Step 700 - Loss: 0.000003\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000003\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000004\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000001\n",
      "Step 1300 - Loss: 0.000006\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000003\n",
      "Step 1450 - Loss: 0.000017\n",
      "Step 1500 - Loss: 0.000008\n",
      "Step 1550 - Loss: 0.000067\n",
      "Step 1600 - Loss: 0.000005\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000008\n",
      "Step 1750 - Loss: 0.000101\n",
      "Step 1800 - Loss: 0.000001\n",
      "Step 1850 - Loss: 0.000058\n",
      "Step 1900 - Loss: 0.000030\n",
      "Step 1950 - Loss: 0.000001\n",
      "Step 2000 - Loss: 0.000005\n",
      "Step 2050 - Loss: 0.000043\n",
      "Step 2100 - Loss: 0.000016\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 17\n",
      "Step 0 - Loss: 0.000588\n",
      "Step 50 - Loss: 0.000011\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000003\n",
      "Step 300 - Loss: 0.000002\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000023\n",
      "Step 450 - Loss: 0.000031\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000004\n",
      "Step 600 - Loss: 0.000002\n",
      "Step 650 - Loss: 0.000001\n",
      "Step 700 - Loss: 0.000003\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000003\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000003\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000001\n",
      "Step 1300 - Loss: 0.000005\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000002\n",
      "Step 1450 - Loss: 0.000017\n",
      "Step 1500 - Loss: 0.000008\n",
      "Step 1550 - Loss: 0.000065\n",
      "Step 1600 - Loss: 0.000005\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000007\n",
      "Step 1750 - Loss: 0.000098\n",
      "Step 1800 - Loss: 0.000001\n",
      "Step 1850 - Loss: 0.000058\n",
      "Step 1900 - Loss: 0.000029\n",
      "Step 1950 - Loss: 0.000001\n",
      "Step 2000 - Loss: 0.000004\n",
      "Step 2050 - Loss: 0.000044\n",
      "Step 2100 - Loss: 0.000015\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 18\n",
      "Step 0 - Loss: 0.000561\n",
      "Step 50 - Loss: 0.000016\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000003\n",
      "Step 300 - Loss: 0.000002\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000022\n",
      "Step 450 - Loss: 0.000030\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000003\n",
      "Step 600 - Loss: 0.000001\n",
      "Step 650 - Loss: 0.000001\n",
      "Step 700 - Loss: 0.000002\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000002\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000003\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000001\n",
      "Step 1300 - Loss: 0.000005\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000002\n",
      "Step 1450 - Loss: 0.000018\n",
      "Step 1500 - Loss: 0.000007\n",
      "Step 1550 - Loss: 0.000063\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000006\n",
      "Step 1750 - Loss: 0.000095\n",
      "Step 1800 - Loss: 0.000001\n",
      "Step 1850 - Loss: 0.000058\n",
      "Step 1900 - Loss: 0.000028\n",
      "Step 1950 - Loss: 0.000001\n",
      "Step 2000 - Loss: 0.000004\n",
      "Step 2050 - Loss: 0.000044\n",
      "Step 2100 - Loss: 0.000015\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 19\n",
      "Step 0 - Loss: 0.000531\n",
      "Step 50 - Loss: 0.000022\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000003\n",
      "Step 300 - Loss: 0.000003\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000021\n",
      "Step 450 - Loss: 0.000028\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000003\n",
      "Step 600 - Loss: 0.000001\n",
      "Step 650 - Loss: 0.000001\n",
      "Step 700 - Loss: 0.000002\n",
      "Step 750 - Loss: 0.000002\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000002\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000003\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000001\n",
      "Step 1300 - Loss: 0.000003\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000002\n",
      "Step 1450 - Loss: 0.000018\n",
      "Step 1500 - Loss: 0.000007\n",
      "Step 1550 - Loss: 0.000059\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000006\n",
      "Step 1750 - Loss: 0.000093\n",
      "Step 1800 - Loss: 0.000001\n",
      "Step 1850 - Loss: 0.000058\n",
      "Step 1900 - Loss: 0.000027\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000003\n",
      "Step 2050 - Loss: 0.000044\n",
      "Step 2100 - Loss: 0.000014\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 20\n",
      "Step 0 - Loss: 0.000500\n",
      "Step 50 - Loss: 0.000028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000003\n",
      "Step 300 - Loss: 0.000003\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000019\n",
      "Step 450 - Loss: 0.000026\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000001\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000003\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000001\n",
      "Step 1300 - Loss: 0.000002\n",
      "Step 1350 - Loss: 0.000001\n",
      "Step 1400 - Loss: 0.000001\n",
      "Step 1450 - Loss: 0.000019\n",
      "Step 1500 - Loss: 0.000007\n",
      "Step 1550 - Loss: 0.000056\n",
      "Step 1600 - Loss: 0.000003\n",
      "Step 1650 - Loss: 0.000004\n",
      "Step 1700 - Loss: 0.000005\n",
      "Step 1750 - Loss: 0.000092\n",
      "Step 1800 - Loss: 0.000001\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000026\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000003\n",
      "Step 2050 - Loss: 0.000045\n",
      "Step 2100 - Loss: 0.000014\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 21\n",
      "Step 0 - Loss: 0.000466\n",
      "Step 50 - Loss: 0.000032\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000003\n",
      "Step 300 - Loss: 0.000003\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000018\n",
      "Step 450 - Loss: 0.000023\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000002\n",
      "Step 950 - Loss: 0.000001\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000003\n",
      "Step 1200 - Loss: 0.000004\n",
      "Step 1250 - Loss: 0.000001\n",
      "Step 1300 - Loss: 0.000001\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000001\n",
      "Step 1450 - Loss: 0.000021\n",
      "Step 1500 - Loss: 0.000006\n",
      "Step 1550 - Loss: 0.000052\n",
      "Step 1600 - Loss: 0.000003\n",
      "Step 1650 - Loss: 0.000004\n",
      "Step 1700 - Loss: 0.000005\n",
      "Step 1750 - Loss: 0.000091\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000025\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000002\n",
      "Step 2050 - Loss: 0.000045\n",
      "Step 2100 - Loss: 0.000012\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 22\n",
      "Step 0 - Loss: 0.000428\n",
      "Step 50 - Loss: 0.000032\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000002\n",
      "Step 300 - Loss: 0.000003\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000017\n",
      "Step 450 - Loss: 0.000020\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000000\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000001\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000003\n",
      "Step 950 - Loss: 0.000001\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000002\n",
      "Step 1200 - Loss: 0.000005\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000001\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000001\n",
      "Step 1450 - Loss: 0.000022\n",
      "Step 1500 - Loss: 0.000006\n",
      "Step 1550 - Loss: 0.000049\n",
      "Step 1600 - Loss: 0.000002\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000005\n",
      "Step 1750 - Loss: 0.000089\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000024\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000002\n",
      "Step 2050 - Loss: 0.000045\n",
      "Step 2100 - Loss: 0.000011\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 23\n",
      "Step 0 - Loss: 0.000385\n",
      "Step 50 - Loss: 0.000028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000002\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000002\n",
      "Step 300 - Loss: 0.000003\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000015\n",
      "Step 450 - Loss: 0.000016\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000000\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000000\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000004\n",
      "Step 950 - Loss: 0.000001\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000002\n",
      "Step 1200 - Loss: 0.000007\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000023\n",
      "Step 1500 - Loss: 0.000006\n",
      "Step 1550 - Loss: 0.000047\n",
      "Step 1600 - Loss: 0.000002\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000005\n",
      "Step 1750 - Loss: 0.000088\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000024\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000001\n",
      "Step 2050 - Loss: 0.000044\n",
      "Step 2100 - Loss: 0.000009\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 24\n",
      "Step 0 - Loss: 0.000334\n",
      "Step 50 - Loss: 0.000021\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000001\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000002\n",
      "Step 300 - Loss: 0.000003\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000013\n",
      "Step 450 - Loss: 0.000013\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000000\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000001\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000006\n",
      "Step 950 - Loss: 0.000000\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000000\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000001\n",
      "Step 1200 - Loss: 0.000008\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000023\n",
      "Step 1500 - Loss: 0.000005\n",
      "Step 1550 - Loss: 0.000045\n",
      "Step 1600 - Loss: 0.000002\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000005\n",
      "Step 1750 - Loss: 0.000087\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000023\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000000\n",
      "Step 2050 - Loss: 0.000042\n",
      "Step 2100 - Loss: 0.000008\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 25\n",
      "Step 0 - Loss: 0.000271\n",
      "Step 50 - Loss: 0.000012\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000001\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000001\n",
      "Step 300 - Loss: 0.000002\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000011\n",
      "Step 450 - Loss: 0.000009\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000000\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000002\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000006\n",
      "Step 950 - Loss: 0.000000\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000001\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000001\n",
      "Step 1200 - Loss: 0.000008\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000022\n",
      "Step 1500 - Loss: 0.000004\n",
      "Step 1550 - Loss: 0.000045\n",
      "Step 1600 - Loss: 0.000002\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000005\n",
      "Step 1750 - Loss: 0.000085\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000023\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000000\n",
      "Step 2050 - Loss: 0.000039\n",
      "Step 2100 - Loss: 0.000006\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 26\n",
      "Step 0 - Loss: 0.000200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 - Loss: 0.000006\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000001\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000001\n",
      "Step 300 - Loss: 0.000002\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000009\n",
      "Step 450 - Loss: 0.000006\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000003\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000006\n",
      "Step 950 - Loss: 0.000000\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000003\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000007\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000020\n",
      "Step 1500 - Loss: 0.000003\n",
      "Step 1550 - Loss: 0.000044\n",
      "Step 1600 - Loss: 0.000002\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000004\n",
      "Step 1750 - Loss: 0.000082\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000058\n",
      "Step 1900 - Loss: 0.000022\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000000\n",
      "Step 2050 - Loss: 0.000035\n",
      "Step 2100 - Loss: 0.000005\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 27\n",
      "Step 0 - Loss: 0.000131\n",
      "Step 50 - Loss: 0.000005\n",
      "Step 100 - Loss: 0.000000\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000001\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000008\n",
      "Step 450 - Loss: 0.000004\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000003\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000006\n",
      "Step 950 - Loss: 0.000000\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000005\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000005\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000017\n",
      "Step 1500 - Loss: 0.000003\n",
      "Step 1550 - Loss: 0.000044\n",
      "Step 1600 - Loss: 0.000002\n",
      "Step 1650 - Loss: 0.000004\n",
      "Step 1700 - Loss: 0.000004\n",
      "Step 1750 - Loss: 0.000079\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000057\n",
      "Step 1900 - Loss: 0.000022\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000001\n",
      "Step 2050 - Loss: 0.000031\n",
      "Step 2100 - Loss: 0.000003\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 28\n",
      "Step 0 - Loss: 0.000076\n",
      "Step 50 - Loss: 0.000005\n",
      "Step 100 - Loss: 0.000000\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000001\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000006\n",
      "Step 450 - Loss: 0.000002\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000003\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000005\n",
      "Step 950 - Loss: 0.000000\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000006\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000014\n",
      "Step 1500 - Loss: 0.000002\n",
      "Step 1550 - Loss: 0.000043\n",
      "Step 1600 - Loss: 0.000002\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000003\n",
      "Step 1750 - Loss: 0.000075\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000056\n",
      "Step 1900 - Loss: 0.000021\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000002\n",
      "Step 2050 - Loss: 0.000026\n",
      "Step 2100 - Loss: 0.000002\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 29\n",
      "Step 0 - Loss: 0.000039\n",
      "Step 50 - Loss: 0.000007\n",
      "Step 100 - Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000005\n",
      "Step 450 - Loss: 0.000001\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000003\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000004\n",
      "Step 950 - Loss: 0.000000\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000002\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000012\n",
      "Step 1500 - Loss: 0.000002\n",
      "Step 1550 - Loss: 0.000042\n",
      "Step 1600 - Loss: 0.000002\n",
      "Step 1650 - Loss: 0.000002\n",
      "Step 1700 - Loss: 0.000003\n",
      "Step 1750 - Loss: 0.000070\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000055\n",
      "Step 1900 - Loss: 0.000019\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000003\n",
      "Step 2050 - Loss: 0.000022\n",
      "Step 2100 - Loss: 0.000002\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 30\n",
      "Step 0 - Loss: 0.000016\n",
      "Step 50 - Loss: 0.000007\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000005\n",
      "Step 450 - Loss: 0.000001\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000004\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000003\n",
      "Step 950 - Loss: 0.000002\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000008\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000001\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000010\n",
      "Step 1500 - Loss: 0.000002\n",
      "Step 1550 - Loss: 0.000041\n",
      "Step 1600 - Loss: 0.000003\n",
      "Step 1650 - Loss: 0.000001\n",
      "Step 1700 - Loss: 0.000002\n",
      "Step 1750 - Loss: 0.000065\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000053\n",
      "Step 1900 - Loss: 0.000018\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000003\n",
      "Step 2050 - Loss: 0.000018\n",
      "Step 2100 - Loss: 0.000001\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 31\n",
      "Step 0 - Loss: 0.000004\n",
      "Step 50 - Loss: 0.000007\n",
      "Step 100 - Loss: 0.000001\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000000\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000004\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000001\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000004\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000002\n",
      "Step 950 - Loss: 0.000004\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000008\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000000\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000009\n",
      "Step 1500 - Loss: 0.000001\n",
      "Step 1550 - Loss: 0.000040\n",
      "Step 1600 - Loss: 0.000003\n",
      "Step 1650 - Loss: 0.000001\n",
      "Step 1700 - Loss: 0.000002\n",
      "Step 1750 - Loss: 0.000059\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000052\n",
      "Step 1900 - Loss: 0.000017\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000004\n",
      "Step 2050 - Loss: 0.000015\n",
      "Step 2100 - Loss: 0.000001\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 32\n",
      "Step 0 - Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 - Loss: 0.000006\n",
      "Step 100 - Loss: 0.000002\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000004\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000004\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000002\n",
      "Step 950 - Loss: 0.000007\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000008\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000000\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000008\n",
      "Step 1500 - Loss: 0.000001\n",
      "Step 1550 - Loss: 0.000038\n",
      "Step 1600 - Loss: 0.000003\n",
      "Step 1650 - Loss: 0.000000\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000054\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000050\n",
      "Step 1900 - Loss: 0.000015\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000005\n",
      "Step 2050 - Loss: 0.000012\n",
      "Step 2100 - Loss: 0.000001\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 33\n",
      "Step 0 - Loss: 0.000001\n",
      "Step 50 - Loss: 0.000005\n",
      "Step 100 - Loss: 0.000003\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000004\n",
      "Step 700 - Loss: 0.000000\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000002\n",
      "Step 950 - Loss: 0.000011\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000008\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000000\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000007\n",
      "Step 1500 - Loss: 0.000001\n",
      "Step 1550 - Loss: 0.000036\n",
      "Step 1600 - Loss: 0.000003\n",
      "Step 1650 - Loss: 0.000000\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000049\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000048\n",
      "Step 1900 - Loss: 0.000014\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000006\n",
      "Step 2050 - Loss: 0.000010\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 34\n",
      "Step 0 - Loss: 0.000006\n",
      "Step 50 - Loss: 0.000004\n",
      "Step 100 - Loss: 0.000004\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000005\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000015\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000008\n",
      "Step 1100 - Loss: 0.000000\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000000\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000007\n",
      "Step 1500 - Loss: 0.000001\n",
      "Step 1550 - Loss: 0.000034\n",
      "Step 1600 - Loss: 0.000003\n",
      "Step 1650 - Loss: 0.000000\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000044\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000046\n",
      "Step 1900 - Loss: 0.000013\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000006\n",
      "Step 2050 - Loss: 0.000008\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 35\n",
      "Step 0 - Loss: 0.000012\n",
      "Step 50 - Loss: 0.000003\n",
      "Step 100 - Loss: 0.000005\n",
      "Step 150 - Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000005\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000000\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000019\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000001\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000001\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000006\n",
      "Step 1500 - Loss: 0.000001\n",
      "Step 1550 - Loss: 0.000032\n",
      "Step 1600 - Loss: 0.000003\n",
      "Step 1650 - Loss: 0.000000\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000040\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000044\n",
      "Step 1900 - Loss: 0.000011\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000007\n",
      "Step 2050 - Loss: 0.000007\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 36\n",
      "Step 0 - Loss: 0.000019\n",
      "Step 50 - Loss: 0.000003\n",
      "Step 100 - Loss: 0.000006\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000006\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000023\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000001\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000001\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000006\n",
      "Step 1500 - Loss: 0.000001\n",
      "Step 1550 - Loss: 0.000030\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000000\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000036\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000042\n",
      "Step 1900 - Loss: 0.000010\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000007\n",
      "Step 2050 - Loss: 0.000006\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 37\n",
      "Step 0 - Loss: 0.000025\n",
      "Step 50 - Loss: 0.000002\n",
      "Step 100 - Loss: 0.000006\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000006\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000026\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000001\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000001\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000006\n",
      "Step 1500 - Loss: 0.000001\n",
      "Step 1550 - Loss: 0.000029\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000001\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000032\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000040\n",
      "Step 1900 - Loss: 0.000010\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000008\n",
      "Step 2050 - Loss: 0.000005\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 38\n",
      "Step 0 - Loss: 0.000031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 - Loss: 0.000002\n",
      "Step 100 - Loss: 0.000006\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000007\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000001\n",
      "Step 950 - Loss: 0.000029\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000001\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000002\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000006\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000027\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000001\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000029\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000039\n",
      "Step 1900 - Loss: 0.000009\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000008\n",
      "Step 2050 - Loss: 0.000004\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 39\n",
      "Step 0 - Loss: 0.000036\n",
      "Step 50 - Loss: 0.000002\n",
      "Step 100 - Loss: 0.000007\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000007\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000031\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000001\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000002\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000026\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000001\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000027\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000037\n",
      "Step 1900 - Loss: 0.000008\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000009\n",
      "Step 2050 - Loss: 0.000003\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 40\n",
      "Step 0 - Loss: 0.000041\n",
      "Step 50 - Loss: 0.000002\n",
      "Step 100 - Loss: 0.000007\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000008\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000033\n",
      "Step 1000 - Loss: 0.000000\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000002\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000002\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000024\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000002\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000024\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000036\n",
      "Step 1900 - Loss: 0.000007\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000009\n",
      "Step 2050 - Loss: 0.000003\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 41\n",
      "Step 0 - Loss: 0.000044\n",
      "Step 50 - Loss: 0.000002\n",
      "Step 100 - Loss: 0.000007\n",
      "Step 150 - Loss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000001\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000008\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000035\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000002\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000002\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000023\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000002\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000022\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000035\n",
      "Step 1900 - Loss: 0.000007\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000009\n",
      "Step 2050 - Loss: 0.000002\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 42\n",
      "Step 0 - Loss: 0.000047\n",
      "Step 50 - Loss: 0.000002\n",
      "Step 100 - Loss: 0.000007\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000008\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000000\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000036\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000002\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000022\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000020\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000033\n",
      "Step 1900 - Loss: 0.000006\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000010\n",
      "Step 2050 - Loss: 0.000002\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 43\n",
      "Step 0 - Loss: 0.000049\n",
      "Step 50 - Loss: 0.000003\n",
      "Step 100 - Loss: 0.000007\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000009\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000037\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000002\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000021\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000003\n",
      "Step 1700 - Loss: 0.000000\n",
      "Step 1750 - Loss: 0.000019\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000032\n",
      "Step 1900 - Loss: 0.000006\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000010\n",
      "Step 2050 - Loss: 0.000002\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 44\n",
      "Step 0 - Loss: 0.000051\n",
      "Step 50 - Loss: 0.000003\n",
      "Step 100 - Loss: 0.000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000009\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000038\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000002\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000020\n",
      "Step 1600 - Loss: 0.000005\n",
      "Step 1650 - Loss: 0.000004\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000017\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000031\n",
      "Step 1900 - Loss: 0.000005\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000010\n",
      "Step 2050 - Loss: 0.000001\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 45\n",
      "Step 0 - Loss: 0.000053\n",
      "Step 50 - Loss: 0.000003\n",
      "Step 100 - Loss: 0.000006\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000000\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000009\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000038\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000003\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000019\n",
      "Step 1600 - Loss: 0.000005\n",
      "Step 1650 - Loss: 0.000004\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000016\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000030\n",
      "Step 1900 - Loss: 0.000005\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000010\n",
      "Step 2050 - Loss: 0.000001\n",
      "Step 2100 - Loss: 0.000000\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 46\n",
      "Step 0 - Loss: 0.000055\n",
      "Step 50 - Loss: 0.000003\n",
      "Step 100 - Loss: 0.000006\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000001\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000010\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000039\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000003\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000018\n",
      "Step 1600 - Loss: 0.000005\n",
      "Step 1650 - Loss: 0.000004\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000015\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000029\n",
      "Step 1900 - Loss: 0.000005\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000011\n",
      "Step 2050 - Loss: 0.000001\n",
      "Step 2100 - Loss: 0.000001\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 47\n",
      "Step 0 - Loss: 0.000056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 - Loss: 0.000004\n",
      "Step 100 - Loss: 0.000006\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000001\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000003\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000010\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000039\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000003\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000005\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000017\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000014\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000028\n",
      "Step 1900 - Loss: 0.000004\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000011\n",
      "Step 2050 - Loss: 0.000001\n",
      "Step 2100 - Loss: 0.000001\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 48\n",
      "Step 0 - Loss: 0.000058\n",
      "Step 50 - Loss: 0.000004\n",
      "Step 100 - Loss: 0.000006\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000001\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000004\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000010\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000039\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000003\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000004\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000017\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000013\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000027\n",
      "Step 1900 - Loss: 0.000004\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000011\n",
      "Step 2050 - Loss: 0.000001\n",
      "Step 2100 - Loss: 0.000001\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n",
      "Epoch 49\n",
      "Step 0 - Loss: 0.000060\n",
      "Step 50 - Loss: 0.000004\n",
      "Step 100 - Loss: 0.000006\n",
      "Step 150 - Loss: 0.000000\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000000\n",
      "Step 300 - Loss: 0.000001\n",
      "Step 350 - Loss: 0.000000\n",
      "Step 400 - Loss: 0.000004\n",
      "Step 450 - Loss: 0.000000\n",
      "Step 500 - Loss: 0.000000\n",
      "Step 550 - Loss: 0.000002\n",
      "Step 600 - Loss: 0.000000\n",
      "Step 650 - Loss: 0.000010\n",
      "Step 700 - Loss: 0.000001\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000001\n",
      "Step 850 - Loss: 0.000001\n",
      "Step 900 - Loss: 0.000000\n",
      "Step 950 - Loss: 0.000039\n",
      "Step 1000 - Loss: 0.000001\n",
      "Step 1050 - Loss: 0.000007\n",
      "Step 1100 - Loss: 0.000004\n",
      "Step 1150 - Loss: 0.000000\n",
      "Step 1200 - Loss: 0.000003\n",
      "Step 1250 - Loss: 0.000000\n",
      "Step 1300 - Loss: 0.000000\n",
      "Step 1350 - Loss: 0.000000\n",
      "Step 1400 - Loss: 0.000000\n",
      "Step 1450 - Loss: 0.000004\n",
      "Step 1500 - Loss: 0.000000\n",
      "Step 1550 - Loss: 0.000016\n",
      "Step 1600 - Loss: 0.000004\n",
      "Step 1650 - Loss: 0.000005\n",
      "Step 1700 - Loss: 0.000001\n",
      "Step 1750 - Loss: 0.000012\n",
      "Step 1800 - Loss: 0.000000\n",
      "Step 1850 - Loss: 0.000026\n",
      "Step 1900 - Loss: 0.000004\n",
      "Step 1950 - Loss: 0.000000\n",
      "Step 2000 - Loss: 0.000011\n",
      "Step 2050 - Loss: 0.000001\n",
      "Step 2100 - Loss: 0.000001\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6414 6417 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6417 6420 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6420 6423 1 3 4\n",
      "[[-0.21741998280605976 0.14285147884497984 -0.04153298939890261\n",
      "  -0.11098711498210005]\n",
      " [-0.21741998280605976 0.012176632219212913 -0.04153298939890261\n",
      "  0.03215804630822253]\n",
      " [-0.21741998280605976 0.001133687433936965 -0.04153298939890261\n",
      "  0.04425482050177093]\n",
      " ..., \n",
      " [0.47732506091309879 -0.1675779690077809 0.2688964584538582\n",
      "  -0.11098711498210005]\n",
      " [0.46862451550577483 -0.11359023894643118 0.21490872839250844\n",
      "  -0.11098711498210005]\n",
      " [-0.20599240077852973 -0.23751661931452944 -0.04153298939890261\n",
      "  0.3056795516845666]] 6423 6426 1 3 4\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "test_pred_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    num_epochs = 50\n",
    "    for epoch_idx in range(num_epochs):\n",
    "                \n",
    "        print('Epoch %d' %epoch_idx)\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length * batch_size\n",
    "        \n",
    "            try:\n",
    "                batchX = xTrain[start_idx:end_idx,:].reshape(batch_size,truncated_backprop_length,num_features)\n",
    "                batchY = yTrain[start_idx:end_idx].reshape(batch_size,truncated_backprop_length,1)\n",
    "            except:\n",
    "                print xTrain, start_idx, end_idx, batch_size, truncated_backprop_length, num_features\n",
    "            #print('IDXs',start_idx,end_idx)\n",
    "            #print('X',batchX.shape,batchX)\n",
    "            #print('Y',batchX.shape,batchY)\n",
    "            \n",
    "            feed = {batchX_placeholder : batchX, batchY_placeholder : batchY}\n",
    "            \n",
    "            #TRAIN!\n",
    "            _loss,_train_step,_pred,_last_label,_prediction = sess.run(\n",
    "                fetches=[loss,train_step,prediction,last_label,prediction],\n",
    "                feed_dict = feed\n",
    "            )\n",
    "            \n",
    "            loss_list.append(_loss)\n",
    "            \n",
    "#             if type(loss) != float(1):\n",
    "#                 print _loss,_train_step,_pred,_last_label,_prediction\n",
    "            \n",
    "           \n",
    "            \n",
    "            if(batch_idx % 50 == 0):\n",
    "                print('Step %d - Loss: %.6f' %(batch_idx,_loss))\n",
    "                \n",
    "    #TEST\n",
    "    \n",
    "    \n",
    "    for test_idx in range(len(xTest) - truncated_backprop_length):\n",
    "        \n",
    "        testBatchX = xTest[test_idx:test_idx+truncated_backprop_length,:].reshape((1,truncated_backprop_length,num_features))        \n",
    "        testBatchY = yTest[test_idx:test_idx+truncated_backprop_length].reshape((1,truncated_backprop_length,1))\n",
    "\n",
    "        \n",
    "        #_current_state = np.zeros((batch_size,state_size))\n",
    "        feed = {batchX_placeholder : testBatchX,\n",
    "            batchY_placeholder : testBatchY}\n",
    "\n",
    "        #Test_pred contains 'window_size' predictions, we want the last one\n",
    "        _last_state,_last_label,test_pred = sess.run([last_state,last_label,prediction],feed_dict=feed)\n",
    "        test_pred_list.append(test_pred[-1][0]) #The last one\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOOO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMsAAAGrCAYAAADeoRBfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f7b9650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(21,7))\n",
    "plt.plot(yTest,label='Price',color='blue')\n",
    "plt.plot(test_pred_list,label='Predicted',color='red')\n",
    "plt.title('Price vs Predicted')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bb3d6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.scatter(x=np.arange(0,len(loss_list)), y=loss_list)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we split the data into disjoint time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     prev_price    compound        neg         neu          pos next_price\n",
      "0     -0.205992    0.342107  -0.041533   -0.237517      0.30568   0.468625\n",
      "1      0.468625    -0.46241   0.214909    -0.11359    -0.110987   0.477325\n",
      "2      0.477325    -0.16998   0.268896   -0.167578    -0.110987   0.475247\n",
      "3      0.475247  -0.0718898  -0.041533    0.142851    -0.110987   0.464296\n",
      "4      0.464296  -0.0718898  -0.041533    0.142851    -0.110987   0.477325\n",
      "5      0.477325  -0.0146195  -0.041533    -0.17494     0.237131   0.472953\n",
      "6      0.472953  -0.0146195   0.203866   -0.368805      0.18068   0.462435\n",
      "7      0.462435  -0.0718898  -0.041533    0.142851    -0.110987   0.472953\n",
      "8      0.472953  -0.0718898  -0.041533    0.142851    -0.110987   0.472953\n",
      "9      0.472953    0.212845  -0.041533   0.0115631    0.0328301   0.457197\n",
      "10     0.457197  -0.0718898  -0.041533    0.142851    -0.110987   0.457197\n",
      "11     0.457197  -0.0718898  -0.041533    0.142851    -0.110987   0.531216\n",
      "12     0.531216    0.152228  -0.041533    0.034876   0.00729245   0.462435\n",
      "13     0.462435  -0.0718898  -0.041533    0.142851    -0.110987   0.477325\n",
      "14     0.477325  -0.0718898  -0.041533    0.142851    -0.110987   0.455639\n",
      "15     0.455639  -0.0718898  -0.041533    0.142851    -0.110987   0.457197\n",
      "16     0.457197  -0.0718898  -0.041533    0.142851    -0.110987   0.457197\n",
      "17     0.457197    0.265988  0.0750314   -0.419112     0.376916   0.462132\n",
      "18     0.462132  -0.0718898  -0.041533    0.142851    -0.110987   0.475247\n",
      "19     0.475247  -0.0718898  -0.041533    0.142851    -0.110987   0.462435\n",
      "20     0.462435  -0.0718898  -0.041533    0.142851    -0.110987   0.457197\n",
      "21     0.457197  -0.0718898  -0.041533    0.142851    -0.110987   0.542125\n",
      "22     0.542125  -0.0718898  -0.041533    0.142851    -0.110987   0.464296\n",
      "23     0.464296  -0.0718898  -0.041533    0.142851    -0.110987   0.461872\n",
      "24     0.461872    0.312886  -0.041533   -0.252241     0.321809   0.457197\n",
      "25     0.457197  -0.0718898  -0.041533    0.142851    -0.110987   0.462132\n",
      "26     0.462132  -0.0718898  -0.041533    0.142851    -0.110987   0.455639\n",
      "27     0.455639  -0.0718898  -0.041533    0.142851    -0.110987   0.477282\n",
      "28     0.477282  -0.0718898  -0.041533    0.142851    -0.110987   0.462435\n",
      "29     0.462435  -0.0718898  -0.041533    0.142851    -0.110987   0.462132\n",
      "...         ...         ...        ...         ...          ...        ...\n",
      "6406  -0.194522   0.0793161  -0.041533    0.042238 -0.000772061  -0.194522\n",
      "6407  -0.194522    0.161653   0.060921   -0.157149     0.106083  -0.194522\n",
      "6408  -0.194522    0.034565  -0.041533     0.10052   -0.0646161  -0.194522\n",
      "6409  -0.194522  -0.0718898  -0.041533    0.142851    -0.110987  -0.194522\n",
      "6410  -0.194522    0.195474  -0.041533  -0.0780074     0.130948  -0.194522\n",
      "6411  -0.194522   0.0463313  -0.041533   0.0207656    0.0227494  -0.194522\n",
      "6412  -0.194522    0.189089  -0.041533  -0.0473326    0.0973462  -0.208676\n",
      "6413  -0.208676   0.0401693  -0.041533   0.0121766     0.032158  -0.208676\n",
      "6414  -0.208676    -0.50323   0.404479   -0.340584   -0.0699925  -0.208676\n",
      "6415  -0.208676   -0.341847   0.285461   -0.184142    -0.110987   -0.20188\n",
      "6416   -0.20188   0.0401693  -0.041533   0.0121766     0.032158   -0.20188\n",
      "6417   -0.20188    0.210949  -0.041533   -0.146106     0.205545   -0.20188\n",
      "6418   -0.20188  -0.0718898  -0.041533    0.142851    -0.110987   -0.21742\n",
      "6419   -0.21742    0.127162  -0.041533  -0.0111363    0.0576957   -0.21742\n",
      "6420   -0.21742  -0.0718898  -0.041533    0.142851    -0.110987   -0.21742\n",
      "6421   -0.21742    -0.23871  0.0198167   0.0815018    -0.110987   -0.21742\n",
      "6422   -0.21742  -0.0338025   0.133927   -0.114817   -0.0209334   -0.21742\n",
      "6423   -0.21742   0.0401693  -0.041533  0.00113369    0.0442548   -0.21742\n",
      "6424   -0.21742   0.0401693  -0.041533   0.0121766     0.032158   -0.21742\n",
      "6425   -0.21742  -0.0718898  -0.041533    0.142851    -0.110987   -0.21742\n",
      "6426   -0.21742   0.0560344  -0.041533   0.0661644   -0.0269817   -0.21742\n",
      "6427   -0.21742     0.20685  -0.041533  -0.0835289     0.136997   -0.21742\n",
      "6428   -0.21742  -0.0718898  -0.041533    0.142851    -0.110987   -0.21742\n",
      "6429   -0.21742 -0.00879205 -0.0188336   0.0845693   -0.0713366  -0.243998\n",
      "6430  -0.243998  -0.0718898  -0.041533    0.142851    -0.110987  -0.243998\n",
      "6431  -0.243998  -0.0975415   0.106933  -0.0939583   -0.0142129  -0.243998\n",
      "6432  -0.243998  -0.0718898  -0.041533    0.142851    -0.110987  -0.243998\n",
      "6433  -0.243998    -0.16669  0.0382216   0.0630969    -0.110987  -0.243998\n",
      "6434  -0.243998   0.0106419  -0.041533   0.0403975   0.00124407  -0.243998\n",
      "6435  -0.243998   -0.134988  0.0652155  -0.0350626   -0.0330301  -0.245556\n",
      "\n",
      "[6426 rows x 6 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "\n",
      "[6426 rows x 0 columns]\n"
     ]
    }
   ],
   "source": [
    "Train2 = dfNN.iloc[:, :4000]\n",
    "Test2 = dfNN.iloc[:, 4000:]\n",
    "print Train2\n",
    "print Test2\n",
    "\n",
    "xTrain2 = Train2[['prev_price','neu','neg','pos']].as_matrix()[::-1]\n",
    "yTrain2 = Train2['next_price'].as_matrix()[::-1]\n",
    "\n",
    "\n",
    "# xTest2 = Test2[['prev_price','neu','neg','pos']].as_matrix()\n",
    "# yTest2 = Test2['next_price'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0 - Loss: 0.117418\n",
      "Step 50 - Loss: 0.003452\n",
      "Step 100 - Loss: 0.000339\n",
      "Step 150 - Loss: 0.000014\n",
      "Step 200 - Loss: 0.000001\n",
      "Step 250 - Loss: 0.000532\n",
      "Step 300 - Loss: 0.000453\n",
      "Step 350 - Loss: 0.000040\n",
      "Step 400 - Loss: 0.001513\n",
      "Step 450 - Loss: 0.000271\n",
      "Step 500 - Loss: 0.000036\n",
      "Step 550 - Loss: 0.000092\n",
      "Step 600 - Loss: 0.000007\n",
      "Step 650 - Loss: 0.000118\n",
      "Step 700 - Loss: 0.000175\n",
      "Step 750 - Loss: 0.000000\n",
      "Step 800 - Loss: 0.000005\n",
      "Step 850 - Loss: 0.000051\n",
      "Step 900 - Loss: 0.000073\n",
      "Step 950 - Loss: 0.000686\n",
      "Step 1000 - Loss: 0.000014\n",
      "Step 1050 - Loss: 0.000012\n",
      "Step 1100 - Loss: 0.000152\n",
      "Step 1150 - Loss: 0.000162\n",
      "Step 1200 - Loss: 0.001079\n",
      "Step 1250 - Loss: 0.000002\n",
      "Step 1300 - Loss: 0.000009\n",
      "Step 1350 - Loss: 0.000046\n",
      "Step 1400 - Loss: 0.000003\n",
      "Step 1450 - Loss: 0.000156\n",
      "Step 1500 - Loss: 0.000157\n",
      "Step 1550 - Loss: 0.006870\n",
      "Step 1600 - Loss: 0.002162\n",
      "Step 1650 - Loss: 0.000116\n",
      "Step 1700 - Loss: 0.000008\n",
      "Step 1750 - Loss: 0.000119\n",
      "Step 1800 - Loss: 0.000012\n",
      "Step 1850 - Loss: 0.000113\n",
      "Step 1900 - Loss: 0.000003\n",
      "Step 1950 - Loss: 0.000223\n",
      "Step 2000 - Loss: 0.000966\n",
      "Step 2050 - Loss: 0.007298\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b4a588b5f1fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             _loss,_train_step,_pred,_last_label,_prediction = sess.run(\n\u001b[1;32m     29\u001b[0m                 \u001b[0mfetches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             )\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "test_pred_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    num_epochs = 10\n",
    "    for epoch_idx in range(num_epochs):\n",
    "                \n",
    "        print('Epoch %d' %epoch_idx)\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length * batch_size\n",
    "        \n",
    "            try:\n",
    "                batchX = xTrain2[start_idx:end_idx,:].reshape(batch_size,truncated_backprop_length,num_features)\n",
    "                batchY = yTrain2[start_idx:end_idx].reshape(batch_size,truncated_backprop_length,1)\n",
    "            except:\n",
    "                print xTrain2, start_idx, end_idx, batch_size, truncated_backprop_length, num_features\n",
    "            #print('IDXs',start_idx,end_idx)\n",
    "            #print('X',batchX.shape,batchX)\n",
    "            #print('Y',batchX.shape,batchY)\n",
    "            \n",
    "            feed = {batchX_placeholder : batchX, batchY_placeholder : batchY}\n",
    "            \n",
    "            #TRAIN!\n",
    "            _loss,_train_step,_pred,_last_label,_prediction = sess.run(\n",
    "                fetches=[loss,train_step,prediction,last_label,prediction],\n",
    "                feed_dict = feed\n",
    "            )\n",
    "            \n",
    "            loss_list.append(_loss)\n",
    "            \n",
    "#             if type(loss) != float(1):\n",
    "#                 print _loss,_train_step,_pred,_last_label,_prediction\n",
    "            \n",
    "           \n",
    "            \n",
    "            if(batch_idx % 50 == 0):\n",
    "                print('Step %d - Loss: %.6f' %(batch_idx,_loss))\n",
    "                \n",
    "    #TEST\n",
    "    \n",
    "    \n",
    "    for test_idx in range(len(xTest2) - truncated_backprop_length):\n",
    "        \n",
    "        try:\n",
    "            testBatchX = xTest2[test_idx:test_idx+truncated_backprop_length,:].reshape((1,truncated_backprop_length,num_features))        \n",
    "            testBatchY = yTest2[test_idx:test_idx+truncated_backprop_length].reshape((1,truncated_backprop_length,1))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #_current_state = np.zeros((batch_size,state_size))\n",
    "        feed = {batchX_placeholder : testBatchX,\n",
    "            batchY_placeholder : testBatchY}\n",
    "\n",
    "        #Test_pred contains 'window_size' predictions, we want the last one\n",
    "        _last_state,_last_label,test_pred = sess.run([last_state,last_label,prediction],feed_dict=feed)\n",
    "        test_pred_list.append(test_pred[-1][0]) #The last one\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMsAAAGrCAYAAADeoRBfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJahbLJEmSJEmSpPYb09XNbQmcnhkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f7b9750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(21,7))\n",
    "plt.plot(yTest2,label='Price',color='blue')\n",
    "plt.plot(test_pred_list,label='Predicted',color='red')\n",
    "plt.title('Price vs Predicted')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
